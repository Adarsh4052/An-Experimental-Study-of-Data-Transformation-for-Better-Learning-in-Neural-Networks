{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre processing\n",
    "This notebook does data preprocessing. Starting with null values removal, then class balancing, then removing high correlated features. Furthermore, it also converts data into 2-d array format for CNNs\n",
    "\n",
    "Dataset link:\n",
    "https://figshare.com/articles/Android_malware_dataset_for_machine_learning_2/5854653/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "# 1 = malware\n",
    "# 0 benign\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "#%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"dataset\\cleaned-data.csv\", sep= \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check for null values...\")\n",
    "print(df.shape)\n",
    "print(df.isnull().sum().tolist())\n",
    "\n",
    "print(\"drop null values...\")\n",
    "df = df.dropna()\n",
    "print(df.shape)\n",
    "print(df.isnull().sum().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use resample method from scikit-learn to sample the data for class balancing\n",
    "from sklearn.utils import resample\n",
    "data = df\n",
    "# Separate majority and minority classes for undersampling\n",
    "df_class1  = data.loc[data['class'] == 1]\n",
    "df_class0   = data[data['class'] == 0]\n",
    "#df.loc[df['A'] == 'foo']\n",
    "df_class1 = resample(df_class1, \n",
    "                                             replace=True,    # sample with replacement\n",
    "                                             n_samples=5555,     # to match number of values in each class\n",
    "                                             random_state=123) # reproducible results\n",
    "\n",
    "df_class0    = resample(df_class0, \n",
    "                                             replace=True,    # sample with replacement\n",
    "                                             n_samples=5555,     # to match number of values in each class\n",
    "                                             random_state=123) # reproducible results\n",
    "\n",
    "# Combine all the class with equal number of values\n",
    "data = pd.concat([ df_class1 , df_class0 ])\n",
    " \n",
    "\n",
    "# check for the class balance\n",
    "print(data['class'].value_counts())\n",
    "# change df with sampled one\n",
    "\n",
    "print(\"\\n\")\n",
    "print(data.shape)\n",
    "print(data.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = data['class']\n",
    "data.drop(['class'] , axis = 1, inplace = True)\n",
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for feature correlation\n",
    "corr = data.corr()\n",
    "plt.figure(figsize=(16, 16))\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ");\n",
    "\n",
    "# check for feature correlation\n",
    "corr = data.iloc[ : , 1:50 ].corr()\n",
    "plt.figure(figsize=(16, 16))\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = data.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.545 )  ]\n",
    "new_data = data.drop(data[to_drop], axis=1, inplace=False)\n",
    "print(\"Data shape = \")\n",
    "print(data.shape)\n",
    "print(\"New data shape = \")\n",
    "print(new_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = new_data\n",
    "wdth = 12\n",
    "hight = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After dropping correlated features \")\n",
    "# check for feature correlation\n",
    "corr = data.corr()\n",
    "plt.figure(figsize=(16, 16))\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ");\n",
    "\n",
    "# check for feature correlation\n",
    "corr = data.iloc[ : , 1:50 ].corr()\n",
    "plt.figure(figsize=(16, 16))\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols= data.columns\n",
    "for col in cols:\n",
    "    print(data[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"dataset\\train_data_straight_rows.csv\"     \n",
    "joblib.dump(data ,filepath )\n",
    "\n",
    "# load model for prediction\n",
    "#data = joblib.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"dataset\\labels.csv\"     \n",
    "joblib.dump(labels ,filepath )\n",
    "\n",
    "# load model for prediction\n",
    "#data = joblib.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    row = np.array(row)\n",
    "    row = row.reshape(wdth,hight)\n",
    "    train_data.append(row)\n",
    "\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = labels\n",
    "\n",
    "for features in train_data:\n",
    "    X.append(features)\n",
    "    \n",
    "X = np.array(X).reshape(-1, wdth,hight, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_siamese = np.array(X).reshape(-1, wdth,hight)\n",
    "X_siamese.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this data format for later use\n",
    "     \n",
    "joblib.dump(X_siamese ,r\"dataset\\X_siamese-11110-12-12-1\" )\n",
    "joblib.dump(labels ,r\"dataset\\labels-11110\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape[1:])\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(X ,r\"dataset\\X_for_cnn\" )\n",
    "joblib.dump(y,r\"dataset\\y\" )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
